{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":52950,"databundleVersionId":5973250,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, sys, platform, subprocess, torch\n!pip -q install --upgrade pip\n!pip -q install pyarrow fastparquet polars pandas\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:12:16.969936Z","iopub.execute_input":"2025-10-15T05:12:16.970686Z","iopub.status.idle":"2025-10-15T05:12:21.764876Z","shell.execute_reply.started":"2025-10-15T05:12:16.970661Z","shell.execute_reply":"2025-10-15T05:12:21.763600Z"}},"outputs":[{"name":"stdout","text":"Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nOS: Linux-6.6.56+-x86_64-with-glibc2.35\nCUDA available: True\nGPU count: 2\nGPU[0]: Tesla T4\nGPU[1]: Tesla T4\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from pathlib import Path\n\nASL_DIR = Path(\"/kaggle/input/asl-fingerspelling\")\nTRAIN_LANDMARKS_DIR = ASL_DIR / \"train_landmarks\"\nSUPP_LANDMARKS_DIR  = ASL_DIR / \"supplemental_landmarks\"\nTRAIN_CSV           = ASL_DIR / \"train.csv\"\nCHARMAP_JSON        = ASL_DIR / \"character_to_prediction_index.json\"\n\nprint(\"ASL_DIR exists:\", ASL_DIR.exists())\nprint(\"TRAIN_LANDMARKS_DIR exists:\", TRAIN_LANDMARKS_DIR.exists())\nprint(\"SUPP_LANDMARKS_DIR exists:\", SUPP_LANDMARKS_DIR.exists())\nprint(\"TRAIN_CSV exists:\", TRAIN_CSV.exists())\nprint(\"CHARMAP_JSON exists:\", CHARMAP_JSON.exists())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:23:16.159525Z","iopub.execute_input":"2025-10-15T05:23:16.160275Z","iopub.status.idle":"2025-10-15T05:23:16.176910Z","shell.execute_reply.started":"2025-10-15T05:23:16.160236Z","shell.execute_reply":"2025-10-15T05:23:16.176270Z"}},"outputs":[{"name":"stdout","text":"ASL_DIR exists: True\nTRAIN_LANDMARKS_DIR exists: True\nSUPP_LANDMARKS_DIR exists: True\nTRAIN_CSV exists: True\nCHARMAP_JSON exists: True\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd, json\n\ndf_train = pd.read_csv(TRAIN_CSV)\nprint(df_train.shape)\ndf_train.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:23:41.460225Z","iopub.execute_input":"2025-10-15T05:23:41.461013Z","iopub.status.idle":"2025-10-15T05:23:41.950005Z","shell.execute_reply.started":"2025-10-15T05:23:41.460969Z","shell.execute_reply":"2025-10-15T05:23:41.949328Z"}},"outputs":[{"name":"stdout","text":"(67208, 5)\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                              path  file_id  sequence_id  participant_id  \\\n0  train_landmarks/5414471.parquet  5414471   1816796431             217   \n1  train_landmarks/5414471.parquet  5414471   1816825349             107   \n2  train_landmarks/5414471.parquet  5414471   1816909464               1   \n\n                phrase  \n0         3 creekhouse  \n1      scales/kuhaylah  \n2  1383 william lanier  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>file_id</th>\n      <th>sequence_id</th>\n      <th>participant_id</th>\n      <th>phrase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816796431</td>\n      <td>217</td>\n      <td>3 creekhouse</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816825349</td>\n      <td>107</td>\n      <td>scales/kuhaylah</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmarks/5414471.parquet</td>\n      <td>5414471</td>\n      <td>1816909464</td>\n      <td>1</td>\n      <td>1383 william lanier</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"with open(CHARMAP_JSON, \"r\") as f:\n    char_map = json.load(f)\nprint(\"Character map size:\", len(char_map))\nlist(char_map.items())[:5]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:24:00.803189Z","iopub.execute_input":"2025-10-15T05:24:00.803817Z","iopub.status.idle":"2025-10-15T05:24:00.813005Z","shell.execute_reply.started":"2025-10-15T05:24:00.803794Z","shell.execute_reply":"2025-10-15T05:24:00.812057Z"}},"outputs":[{"name":"stdout","text":"Character map size: 59\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[(' ', 0), ('!', 1), ('#', 2), ('$', 3), ('%', 4)]"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import polars as pl\nfrom glob import glob\n\ntrain_files = sorted(glob(str(TRAIN_LANDMARKS_DIR / \"*.parquet\")))[:2]\nsupp_files  = sorted(glob(str(SUPP_LANDMARKS_DIR  / \"*.parquet\")))[:2]\n\nprint(\"Sample train_landmarks files:\", train_files)\nprint(\"Sample supplemental_landmarks files:\", supp_files)\n\nif train_files:\n    df_landmarks = pl.read_parquet(train_files[0], n_rows=1000)\n    print(\"train_landmarks sample:\", df_landmarks.shape)\n    print(df_landmarks.head(3))\n\nif supp_files:\n    df_supp = pl.read_parquet(supp_files[0], n_rows=1000)\n    print(\"supplemental_landmarks sample:\", df_supp.shape)\n    print(df_supp.head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:24:59.289160Z","iopub.execute_input":"2025-10-15T05:24:59.289981Z","iopub.status.idle":"2025-10-15T05:25:11.549891Z","shell.execute_reply.started":"2025-10-15T05:24:59.289953Z","shell.execute_reply":"2025-10-15T05:25:11.549200Z"}},"outputs":[{"name":"stdout","text":"Sample train_landmarks files: ['/kaggle/input/asl-fingerspelling/train_landmarks/1019715464.parquet', '/kaggle/input/asl-fingerspelling/train_landmarks/1021040628.parquet']\nSample supplemental_landmarks files: ['/kaggle/input/asl-fingerspelling/supplemental_landmarks/1032110484.parquet', '/kaggle/input/asl-fingerspelling/supplemental_landmarks/1047404576.parquet']\ntrain_landmarks sample: (1000, 1631)\nshape: (3, 1_631)\n┌───────┬──────────┬──────────┬──────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n│ frame ┆ x_face_0 ┆ x_face_1 ┆ x_face_2 ┆ … ┆ z_right_han ┆ z_right_han ┆ z_right_ha ┆ sequence_i │\n│ ---   ┆ ---      ┆ ---      ┆ ---      ┆   ┆ d_18        ┆ d_19        ┆ nd_20      ┆ d          │\n│ i16   ┆ f32      ┆ f32      ┆ f32      ┆   ┆ ---         ┆ ---         ┆ ---        ┆ ---        │\n│       ┆          ┆          ┆          ┆   ┆ f32         ┆ f32         ┆ f32        ┆ i64        │\n╞═══════╪══════════╪══════════╪══════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n│ 0     ┆ 0.578892 ┆ 0.578482 ┆ 0.582906 ┆ … ┆ -0.187797   ┆ -0.224827   ┆ -0.249662  ┆ 1975433633 │\n│ 1     ┆ 0.577563 ┆ 0.578528 ┆ 0.582916 ┆ … ┆ null        ┆ null        ┆ null       ┆ 1975433633 │\n│ 2     ┆ 0.576181 ┆ 0.576949 ┆ 0.581346 ┆ … ┆ -0.173169   ┆ -0.200727   ┆ -0.219106  ┆ 1975433633 │\n└───────┴──────────┴──────────┴──────────┴───┴─────────────┴─────────────┴────────────┴────────────┘\nsupplemental_landmarks sample: (1000, 1631)\nshape: (3, 1_631)\n┌───────┬──────────┬──────────┬──────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n│ frame ┆ x_face_0 ┆ x_face_1 ┆ x_face_2 ┆ … ┆ z_right_han ┆ z_right_han ┆ z_right_ha ┆ sequence_i │\n│ ---   ┆ ---      ┆ ---      ┆ ---      ┆   ┆ d_18        ┆ d_19        ┆ nd_20      ┆ d          │\n│ i16   ┆ f32      ┆ f32      ┆ f32      ┆   ┆ ---         ┆ ---         ┆ ---        ┆ ---        │\n│       ┆          ┆          ┆          ┆   ┆ f32         ┆ f32         ┆ f32        ┆ i64        │\n╞═══════╪══════════╪══════════╪══════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n│ 0     ┆ null     ┆ null     ┆ null     ┆ … ┆ null        ┆ null        ┆ null       ┆ 1617884228 │\n│ 3     ┆ null     ┆ null     ┆ null     ┆ … ┆ null        ┆ null        ┆ null       ┆ 1617884228 │\n│ 4     ┆ null     ┆ null     ┆ null     ┆ … ┆ null        ┆ null        ┆ null       ┆ 1617884228 │\n└───────┴──────────┴──────────┴──────────┴───┴─────────────┴─────────────┴────────────┴────────────┘\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from __future__ import annotations\nimport os, json, math, re, gc\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\n\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nimport warnings; \nwarnings.filterwarnings(\"ignore\", category=FutureWarning); \nwarnings.filterwarnings(\"ignore\", message=\"Resolving the schema of a LazyFrame\")\n\nDATA_DIR = Path(\"/kaggle/input/asl-fingerspelling\")\nTRAIN_LANDMARKS = DATA_DIR / \"train_landmarks\"\nSUPP_LANDMARKS = DATA_DIR / \"supplemental_landmarks\"\nTRAIN_CSV = DATA_DIR / \"train.csv\"\nOUT_DIR = Path(\"/kaggle/working/asl_phase1\")\nSEQS_DIR = OUT_DIR / \"sequences\"\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nSEQS_DIR.mkdir(parents=True, exist_ok=True)\n\nPOSE_KEEP = [11, 12, 13, 14, 15, 16]\n\nHAND_PREFIXES = [\n    (\"left_hand\", list(range(21))),\n    (\"right_hand\", list(range(21))),\n]\n\nCOL_CACHE: Dict[str, List[str]] = {}\n\ndef discover_columns(parquet_path: Path) -> List[str]:\n    if parquet_path.suffix.lower() != \".parquet\":\n        raise ValueError(\"Expected a .parquet file\")\n    df_schema = pl.scan_parquet(str(parquet_path)).schema\n    return list(df_schema.keys())\n\n_XYZ = [\"x\", \"y\", \"z\"]\n\ndef _landmark_cols(prefix: str, idx: int, cols: List[str]) -> List[str]:\n    cands = []\n    for axis in _XYZ:\n        patts = [\n            fr\"^{axis}_{re.escape(prefix)}_{idx}$\",\n            fr\"^{re.escape(prefix)}_{axis}_{idx}$\",\n            fr\"^{axis}_(?:hand_)?{re.escape(prefix)}_{idx}$\",\n            fr\"^{axis}_{re.escape(prefix)}{idx}$\",\n        ]\n        for p in patts:\n            found = [c for c in cols if re.match(p, c)]\n            if found:\n                cands.append(found[0])\n                break\n    return cands if len(cands) == 3 else []\n\n\ndef select_feature_columns(cols: List[str]) -> Tuple[List[str], Dict[str, Tuple[str,str,str]]]:\n    logical2cols: Dict[str, Tuple[str,str,str]] = {}\n\n    for hand, idxs in HAND_PREFIXES:\n        for i in idxs:\n            triple = _landmark_cols(hand, i, cols)\n            if triple:\n                logical2cols[f\"{hand}_{i}\"] = tuple(triple)  # type: ignore\n\n    for i in POSE_KEEP:\n        triple = _landmark_cols(\"pose\", i, cols)\n        if triple:\n            logical2cols[f\"pose_{i}\"] = tuple(triple)  # type: ignore\n\n    ordered = [k for k in [*(f\"pose_{i}\" for i in POSE_KEEP),\n                           *(f\"left_hand_{i}\" for i in range(21)),\n                           *(f\"right_hand_{i}\" for i in range(21))] if k in logical2cols]\n    feat_cols = list(itertools.chain.from_iterable([list(logical2cols[k]) for k in ordered]))\n    return feat_cols, logical2cols\n\nimport itertools\n\ndef interpolate_groupwise(df_pd: pd.DataFrame, group_col: str, value_cols: List[str]) -> pd.DataFrame:\n    def _interp(g: pd.DataFrame) -> pd.DataFrame:\n        g = g.sort_values(\"frame\").copy()\n        g[value_cols] = g[value_cols].astype(\"float32\")\n        g[value_cols] = g[value_cols].interpolate(method=\"linear\", limit_direction=\"both\", axis=0)\n        g[value_cols] = g[value_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n        return g\n    return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n\n\ndef compute_reference_points(df_pl: pl.DataFrame, logical2cols: Dict[str, Tuple[str,str,str]]):\n    has_pose = all(f\"pose_{i}\" in logical2cols for i in [11,12])\n    def xyz_expr(key: str):\n        xs, ys, zs = logical2cols[key]\n        return pl.struct([\n            pl.col(xs).alias(\"x\"),\n            pl.col(ys).alias(\"y\"),\n            pl.col(zs).alias(\"z\"),\n        ])\n\n    if has_pose:\n        lsh = xyz_expr(\"pose_11\")\n        rsh = xyz_expr(\"pose_12\")\n        center = (lsh + rsh) / 2  # struct arithmetic works elementwise in Polars >=0.20\n        shoulder_dist = ((lsh.struct.field(\"x\") - rsh.struct.field(\"x\")).pow(2)\n                         + (lsh.struct.field(\"y\") - rsh.struct.field(\"y\")).pow(2)\n                         + (lsh.struct.field(\"z\") - rsh.struct.field(\"z\")).pow(2)).sqrt()\n        ref = df_pl.with_columns([\n            center.struct.field(\"x\").alias(\"cx\"),\n            center.struct.field(\"y\").alias(\"cy\"),\n            center.struct.field(\"z\").alias(\"cz\"),\n            shoulder_dist.alias(\"scale\"),\n        ])\n    else:\n        lw = xyz_expr(\"left_hand_0\") if \"left_hand_0\" in logical2cols else None\n        rw = xyz_expr(\"right_hand_0\") if \"right_hand_0\" in logical2cols else None\n        if lw is None and rw is None:\n            ref = df_pl.with_columns([\n                pl.lit(0.0).alias(\"cx\"), pl.lit(0.0).alias(\"cy\"), pl.lit(0.0).alias(\"cz\"), pl.lit(1e-6).alias(\"scale\")\n            ])\n        else:\n            if lw is None: lw = rw\n            if rw is None: rw = lw\n            center = (lw + rw) / 2\n            hand_span = ((lw.struct.field(\"x\") - rw.struct.field(\"x\")).pow(2)\n                         + (lw.struct.field(\"y\") - rw.struct.field(\"y\")).pow(2)\n                         + (lw.struct.field(\"z\") - rw.struct.field(\"z\")).pow(2)).sqrt()\n            ref = df_pl.with_columns([\n                center.struct.field(\"x\").alias(\"cx\"),\n                center.struct.field(\"y\").alias(\"cy\"),\n                center.struct.field(\"z\").alias(\"cz\"),\n                hand_span.alias(\"scale\"),\n            ])\n\n    ref = ref.with_columns([\n        pl.when(pl.col(\"scale\") < 1e-6).then(1e-6).otherwise(pl.col(\"scale\")).alias(\"scale\")\n    ])\n    return ref.select([\"cx\",\"cy\",\"cz\",\"scale\"])  \n\n\ndef normalize_features(df_pl: pl.DataFrame, feat_cols: List[str], logical2cols: Dict[str, Tuple[str,str,str]]):\n    ref = compute_reference_points(df_pl, logical2cols)\n    df = pl.concat([df_pl, ref], how=\"horizontal\")\n    out_cols = []\n    for i in range(0, len(feat_cols), 3):\n        x, y, z = feat_cols[i:i+3]\n        nx = ((pl.col(x) - pl.col(\"cx\")) / pl.col(\"scale\")).alias(f\"n_{x}\")\n        ny = ((pl.col(y) - pl.col(\"cy\")) / pl.col(\"scale\")).alias(f\"n_{y}\")\n        nz = ((pl.col(z) - pl.col(\"cz\")) / pl.col(\"scale\")).alias(f\"n_{z}\")\n        out_cols += [nx, ny, nz]\n    norm = df.select([\"sequence_id\",\"frame\", *out_cols])\n    return norm\n\ndef list_parquets() -> List[Path]:\n    files = []\n    if TRAIN_LANDMARKS.exists():\n        files += sorted(TRAIN_LANDMARKS.glob(\"*.parquet\"))\n    if SUPP_LANDMARKS.exists():\n        files += sorted(SUPP_LANDMARKS.glob(\"*.parquet\"))\n    return files\n\n\ndef process_parquet(pq_path: Path) -> Dict:\n    cols = discover_columns(pq_path)\n    meta_cols = [c for c in (\"frame\",\"sequence_id\",\"participant_id\",\"signer_id\",\"hand\") if c in cols]\n    feat_cols, logical2cols = select_feature_columns(cols)\n    if not feat_cols:\n        return {\"ok\": False, \"path\": str(pq_path), \"reason\": \"No matching landmark columns\"}\n\n    df = pl.read_parquet(str(pq_path), columns=meta_cols + feat_cols)\n\n    df = df.with_columns([\n        pl.all().exclude([\"sequence_id\",\"participant_id\",\"signer_id\",\"hand\"]).cast(pl.Float32, strict=False)\n    ])\n\n    norm = normalize_features(df, feat_cols, logical2cols)\n\n    norm_pd = norm.to_pandas()\n    value_cols = [c for c in norm_pd.columns if c not in (\"sequence_id\",\"frame\")]\n    norm_pd = interpolate_groupwise(norm_pd, group_col=\"sequence_id\", value_cols=value_cols)\n\n    groups = norm_pd.groupby(\"sequence_id\")\n    rows = []\n    for seq_id, g in groups:\n        g = g.sort_values(\"frame\")\n        frames = g[value_cols].to_numpy(dtype=np.float32)\n        fidx = g[\"frame\"].astype(np.int16).to_numpy()\n        out_path = SEQS_DIR / f\"{seq_id}.npz\"\n        np.savez_compressed(out_path, frames=frames, frame_index=fidx)\n        rows.append({\n            \"sequence_id\": int(seq_id),\n            \"n_frames\": int(frames.shape[0]),\n            \"n_features\": int(frames.shape[1]),\n            \"npz_path\": str(out_path),\n            \"source\": str(pq_path),\n        })\n    del df, norm, norm_pd\n    gc.collect()\n    return {\"ok\": True, \"path\": str(pq_path), \"n_sequences\": len(rows), \"rows\": rows}\n\n\ndef build_manifest(results: List[Dict]):\n    all_rows = list(itertools.chain.from_iterable([r.get(\"rows\", []) for r in results if r.get(\"ok\")]))\n    man = pd.DataFrame(all_rows)\n    man.to_csv(OUT_DIR / \"manifest.csv\", index=False)\n    if all_rows:\n        pass\n        \ndef main(max_files: int | None = 10):\n    files = list_parquets()\n    if max_files is not None:\n        files = files[:max_files]\n    results = []\n    for i, pq in enumerate(files, 1):\n        print(f\"[{i}/{len(files)}] Processing {pq.name}…\")\n        res = process_parquet(pq)\n        if res.get(\"ok\"):\n            print(f\"  -> OK: {res['n_sequences']} sequences\")\n        else:\n            print(f\"  -> SKIP: {res.get('reason')}\")\n        results.append(res)\n    build_manifest(results)\n    print(\"Done. Manifest at:\", OUT_DIR / \"manifest.csv\")\n\n\nif __name__ == \"__main__\":\n    main(max_files=None) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T06:29:30.636121Z","iopub.execute_input":"2025-10-15T06:29:30.636810Z"}},"outputs":[{"name":"stdout","text":"[1/121] Processing 1019715464.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[2/121] Processing 1021040628.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[3/121] Processing 105143404.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[4/121] Processing 1098899348.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[5/121] Processing 1099408314.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[6/121] Processing 1133664520.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[7/121] Processing 1134756332.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[8/121] Processing 1255240050.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[9/121] Processing 128822441.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[10/121] Processing 1320204318.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[11/121] Processing 1341528257.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[12/121] Processing 1358493307.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[13/121] Processing 1365275733.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 995 sequences\n[14/121] Processing 1365772051.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 997 sequences\n[15/121] Processing 1405046009.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[16/121] Processing 1448136004.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[17/121] Processing 1497621680.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 997 sequences\n[18/121] Processing 149822653.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[19/121] Processing 152029243.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[20/121] Processing 1552432300.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[21/121] Processing 1557244878.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 997 sequences\n[22/121] Processing 1562234637.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 997 sequences\n[23/121] Processing 1643479812.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[24/121] Processing 1647220008.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[25/121] Processing 1662742697.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[26/121] Processing 1664666588.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[27/121] Processing 169560558.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[28/121] Processing 1726141437.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[29/121] Processing 175396851.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[30/121] Processing 1785039512.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[31/121] Processing 1865557033.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[32/121] Processing 1880177496.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[33/121] Processing 1905462118.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[34/121] Processing 1906357076.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 997 sequences\n[35/121] Processing 1920330615.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[36/121] Processing 1967755728.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[37/121] Processing 1969985709.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[38/121] Processing 1997878546.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[39/121] Processing 2026717426.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[40/121] Processing 2036580525.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[41/121] Processing 2072296290.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[42/121] Processing 2072876091.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 997 sequences\n[43/121] Processing 2118949241.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[44/121] Processing 234418913.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[45/121] Processing 296317215.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[46/121] Processing 349393104.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[47/121] Processing 388576474.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[48/121] Processing 425182931.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 995 sequences\n[49/121] Processing 433948159.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[50/121] Processing 450474571.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 287 sequences\n[51/121] Processing 474255203.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[52/121] Processing 495378749.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[53/121] Processing 522550314.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[54/121] Processing 527708222.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[55/121] Processing 532011803.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[56/121] Processing 5414471.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[57/121] Processing 546816846.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[58/121] Processing 566963657.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[59/121] Processing 568753759.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 998 sequences\n[60/121] Processing 614661748.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 996 sequences\n[61/121] Processing 638508439.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[62/121] Processing 649779897.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[63/121] Processing 654436541.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[64/121] Processing 683666742.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[65/121] Processing 871280215.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[66/121] Processing 882979387.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 997 sequences\n[67/121] Processing 933868835.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[68/121] Processing 939623093.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[69/121] Processing 1032110484.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[70/121] Processing 1047404576.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[71/121] Processing 1112747136.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[72/121] Processing 1118603411.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[73/121] Processing 1144115867.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[74/121] Processing 1176508147.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[75/121] Processing 1249944812.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 999 sequences\n[76/121] Processing 1279694894.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[77/121] Processing 131312512.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[78/121] Processing 1407656790.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[79/121] Processing 1471096258.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[80/121] Processing 1471341722.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[81/121] Processing 1505488209.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[82/121] Processing 1579345709.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[83/121] Processing 1624527344.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[84/121] Processing 1650637630.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 958 sequences\n[85/121] Processing 1682915129.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[86/121] Processing 1727438550.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[87/121] Processing 1755047076.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[88/121] Processing 1756773911.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[89/121] Processing 1779786322.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[90/121] Processing 1857374937.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[91/121] Processing 1881515495.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[92/121] Processing 193950599.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[93/121] Processing 2057261717.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[94/121] Processing 2100073719.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[95/121] Processing 236903981.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[96/121] Processing 285528514.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[97/121] Processing 293101677.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[98/121] Processing 333606065.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[99/121] Processing 33432165.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[100/121] Processing 369584223.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[101/121] Processing 371169664.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[102/121] Processing 440362090.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[103/121] Processing 442061898.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[104/121] Processing 471766624.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[105/121] Processing 595441814.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[106/121] Processing 597469033.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[107/121] Processing 636900267.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[108/121] Processing 639454452.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[109/121] Processing 676340265.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[110/121] Processing 680303484.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[111/121] Processing 697480828.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[112/121] Processing 716508881.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[113/121] Processing 736978972.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[114/121] Processing 756566775.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[115/121] Processing 775880548.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[116/121] Processing 778903889.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"},{"name":"stdout","text":"  -> OK: 1000 sequences\n[117/121] Processing 86446671.parquet…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/3718272648.py:131: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  return df_pd.groupby(group_col, as_index=False, group_keys=False).apply(_interp)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport pandas as pd, numpy as np, os, json\n\nASL_DIR  = Path(\"/kaggle/input/asl-fingerspelling\")\nOUT_DIR  = Path(\"/kaggle/working/asl_phase1\")\nSEQS_DIR = OUT_DIR / \"sequences\"\nTRAIN_CSV    = ASL_DIR / \"train.csv\"\n\nassert SEQS_DIR.exists(), f\"{SEQS_DIR} missing — run Phase 1 exporter first\"\nassert TRAIN_CSV.exists(), f\"{TRAIN_CSV} missing\"\n\ndf_train = pd.read_csv(TRAIN_CSV)[[\"sequence_id\",\"phrase\"]]\ndf_train[\"sequence_id\"] = df_train[\"sequence_id\"].astype(int)\n\nrows = []\nnpz_files = sorted(SEQS_DIR.glob(\"*.npz\"))\nprint(f\"Found {len(npz_files)} sequence artifacts under {SEQS_DIR}\")\n\nfor f in npz_files:\n    s = f.stem\n    if not s.isdigit():\n        continue\n    sid = int(s)\n    try:\n        with np.load(f, allow_pickle=False) as z:\n            arr = z[\"frames\"]\n            T, F = int(arr.shape[0]), int(arr.shape[1])\n    except Exception as e:\n        T, F = np.nan, np.nan\n    rows.append({\"sequence_id\": sid, \"npz_path\": str(f), \"n_frames\": T, \"n_features\": F})\n\nman_all = pd.DataFrame(rows).drop_duplicates(\"sequence_id\").reset_index(drop=True)\nman_all = man_all.merge(df_train, on=\"sequence_id\", how=\"left\")\nman_all[\"has_label\"] = man_all[\"phrase\"].notna()\n\nOUT_DIR.mkdir(parents=True, exist_ok=True)\nman_all.to_csv(OUT_DIR / \"manifest_all.csv\", index=False)\n\nman = man_all[man_all[\"has_label\"]].copy()\nman = man.drop(columns=[\"has_label\"]).reset_index(drop=True)\nman.to_csv(OUT_DIR / \"manifest.csv\", index=False)\n\nprint(f\"manifest_all.csv: {len(man_all)} rows  (labeled: {man_all['has_label'].sum()})\")\nprint(f\"manifest.csv (labeled only): {len(man)} rows  -> {OUT_DIR/'manifest.csv'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:51:59.061317Z","iopub.execute_input":"2025-10-15T10:51:59.061990Z","iopub.status.idle":"2025-10-15T10:53:29.025344Z","shell.execute_reply.started":"2025-10-15T10:51:59.061966Z","shell.execute_reply":"2025-10-15T10:53:29.024545Z"}},"outputs":[{"name":"stdout","text":"Found 120165 sequence artifacts under /kaggle/working/asl_phase1/sequences\n✅ manifest_all.csv: 120165 rows  (labeled: 67208)\n✅ manifest.csv (labeled only): 67208 rows  -> /kaggle/working/asl_phase1/manifest.csv\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"import os, math, time, json, random\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import List\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.set_float32_matmul_precision(\"medium\")  # enable TF32 where available\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nASL_DIR  = Path(\"/kaggle/input/asl-fingerspelling\")\nOUT_DIR  = Path(\"/kaggle/working/asl_phase1\")\nSEQS_DIR = OUT_DIR / \"sequences\"\nMANIFEST_ALL = OUT_DIR / \"manifest_all.csv\"\nMANIFEST = OUT_DIR / \"manifest.csv\"\nCHARMAP_JSON = ASL_DIR / \"character_to_prediction_index.json\"\n\nassert MANIFEST.exists(), \"Run the manifest rebuild cell first.\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:53:39.029582Z","iopub.execute_input":"2025-10-15T10:53:39.030306Z","iopub.status.idle":"2025-10-15T10:53:39.035832Z","shell.execute_reply.started":"2025-10-15T10:53:39.030250Z","shell.execute_reply":"2025-10-15T10:53:39.035121Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"with open(CHARMAP_JSON, \"r\") as f:\n    base_char2idx = json.load(f)\n\nPAD, BOS, EOS = \"<pad>\", \"<bos>\", \"<eos>\"\n\nhas_pad = (PAD in base_char2idx) or (0 in set(base_char2idx.values()))\nif has_pad and (PAD in base_char2idx):\n    char2idx = dict(base_char2idx)\n    char2idx[PAD] = base_char2idx[PAD]\n    start_max = max(char2idx.values())\nelse:\n    min_id = min(base_char2idx.values())\n    shift = 1 if min_id == 0 else 0\n    char2idx = {c: i+shift for c, i in base_char2idx.items()}\n    char2idx[PAD] = 0\n    start_max = max(char2idx.values())\n\nif BOS not in char2idx: char2idx[BOS] = start_max + 1; start_max += 1\nif EOS not in char2idx: char2idx[EOS] = start_max + 1\n\nidx2char = {i: c for c, i in char2idx.items()}\nvocab_size = max(char2idx.values()) + 1\n\ndef text_to_ids(s: str) -> List[int]:\n    return [char2idx[BOS]] + [char2idx[c] for c in s.lower() if c in char2idx] + [char2idx[EOS]]\n\ndef ids_to_text(ids: List[int]) -> str:\n    return \"\".join(idx2char[i] for i in ids if i not in (char2idx[PAD], char2idx[BOS], char2idx[EOS]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:53:46.315101Z","iopub.execute_input":"2025-10-15T10:53:46.315356Z","iopub.status.idle":"2025-10-15T10:53:46.325794Z","shell.execute_reply.started":"2025-10-15T10:53:46.315338Z","shell.execute_reply":"2025-10-15T10:53:46.325105Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"df = pd.read_csv(MANIFEST)\nneed = {\"sequence_id\",\"npz_path\",\"n_frames\",\"n_features\",\"phrase\"}\nmissing = need - set(df.columns)\nassert not missing, f\"Manifest missing columns: {missing}\"\n\nmissing_mask = ~df[\"npz_path\"].map(os.path.exists)\nif missing_mask.any():\n    print(f\"Dropping {int(missing_mask.sum())} rows with missing files\")\n    df = df[~missing_mask].reset_index(drop=True)\n\ndf = df.sort_values(\"n_frames\").reset_index(drop=True)\nlens = df[\"n_frames\"].to_numpy()\nquant = np.quantile(lens, [0.2, 0.4, 0.6, 0.8]) if len(df) > 10 else [0,0,0,0]\ndef bucket(l):\n    return int(l > quant[0]) + int(l > quant[1]) + int(l > quant[2]) + int(l > quant[3])\ndf[\"len_bucket\"] = [bucket(l) for l in lens]\n\nrng = np.random.default_rng(42)\ntrain_idx, val_idx = [], []\nfor _, grp in df.groupby(\"len_bucket\", group_keys=False):\n    idx = grp.index.to_numpy()\n    rng.shuffle(idx)\n    cut = max(1, int(0.90 * len(idx)))\n    train_idx += idx[:cut].tolist()\n    val_idx   += idx[cut:].tolist()\n\n@dataclass\nclass Sample:\n    npz_path: str\n    n_frames: int\n    phrase: str\n    seq_id: int\n\nclass ASLFingerSeqDataset(torch.utils.data.Dataset):\n    def __init__(self, frame_rows: pd.DataFrame):\n        self.rows = [\n            Sample(r[\"npz_path\"], int(r[\"n_frames\"]), str(r[\"phrase\"]), int(r[\"sequence_id\"]))\n            for _, r in frame_rows.iterrows()\n        ]\n    def __len__(self): return len(self.rows)\n    def __getitem__(self, i: int):\n        r = self.rows[i]; p = r.npz_path\n        with np.load(p, allow_pickle=False) as z:\n            frames = z[\"frames\"]  # (T, F)\n        if frames.ndim != 2:\n            raise RuntimeError(f\"Bad ndim={frames.ndim} in {p}\")\n        frames = frames.astype(np.float32, copy=False)\n        if not np.isfinite(frames).all():\n            np.nan_to_num(frames, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n        frames = np.clip(frames, -1e6, 1e6, out=frames)\n        frames_t = torch.from_numpy(frames).contiguous()\n        tgt_ids  = torch.as_tensor(text_to_ids(r.phrase), dtype=torch.long)\n        return frames_t, tgt_ids, r.n_frames, r.seq_id\n\nds_train = ASLFingerSeqDataset(df.loc[train_idx])\nds_val   = ASLFingerSeqDataset(df.loc[val_idx])\n\nclass BucketedBatchSampler(torch.utils.data.Sampler):\n    def __init__(self, lengths, batch_size, shuffle=True):\n        self.batch_size = batch_size; self.shuffle = shuffle\n        order = np.argsort(lengths)\n        if shuffle:\n            chunks = np.array_split(order, max(1, len(order)//(batch_size*20)))\n            rng2 = np.random.default_rng(0)\n            order = np.concatenate([rng2.choice(c, size=len(c), replace=False) for c in chunks if len(c)>0])\n        self.batches = [order[i:i+batch_size] for i in range(0, len(order), batch_size)]\n    def __iter__(self):\n        if self.shuffle:\n            rng3 = np.random.default_rng(1)\n            rng3.shuffle(self.batches)\n        return iter(self.batches)\n    def __len__(self): return len(self.batches)\n\ndef collate_fn(batch):\n    frames, tgts, lens, seq_ids = zip(*batch)\n    B = len(batch); Fdim = frames[0].shape[1]\n    T = max(x.shape[0] for x in frames)\n    L = max(y.shape[0] for y in tgts)\n\n    xpad = torch.zeros(B, T, Fdim, dtype=torch.float32)\n    xmask = torch.ones(B, T, dtype=torch.bool)\n    for i, x in enumerate(frames):\n        t = x.shape[0]\n        xpad[i, :t] = x\n        xmask[i, :t] = False\n\n    ypad = torch.full((B, L), fill_value=char2idx[PAD], dtype=torch.long)\n    ymask = torch.ones(B, L, dtype=torch.bool)\n    for i, y in enumerate(tgts):\n        l = y.shape[0]\n        ypad[i, :l] = y\n        ymask[i, :l] = False\n\n    return xpad, xmask, ypad, ymask, torch.tensor(lens), torch.tensor(seq_ids)\n\nBASE_BATCH = 64\nn_features = int(df[\"n_features\"].dropna().iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:53:57.976986Z","iopub.execute_input":"2025-10-15T10:53:57.977274Z","iopub.status.idle":"2025-10-15T10:54:01.249865Z","shell.execute_reply.started":"2025-10-15T10:53:57.977236Z","shell.execute_reply":"2025-10-15T10:54:01.249273Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"import multiprocessing as mp\nworld_size = torch.cuda.device_count() if torch.cuda.is_available() else 1\n\nBATCH_SIZE = max(1, BASE_BATCH * world_size)  \n\ntrain_sampler = BucketedBatchSampler(df.loc[train_idx, \"n_frames\"].to_numpy(), BATCH_SIZE, shuffle=True)\nval_sampler   = BucketedBatchSampler(df.loc[val_idx,   \"n_frames\"].to_numpy(), BATCH_SIZE, shuffle=False)\n\nnum_workers     = 4\nuse_persistent  = True\nprefetch_factor = 4\npin_mem         = True\ntimeout_seconds = 0  \nmp_ctx = mp.get_context(\"fork\")  \n\ndl_train = torch.utils.data.DataLoader(\n    ds_train, batch_sampler=train_sampler,\n    num_workers=num_workers, pin_memory=pin_mem, collate_fn=collate_fn,\n    persistent_workers=use_persistent, prefetch_factor=prefetch_factor,\n    timeout=timeout_seconds, multiprocessing_context=mp_ctx\n)\ndl_val = torch.utils.data.DataLoader(\n    ds_val, batch_sampler=val_sampler,\n    num_workers=num_workers, pin_memory=pin_mem, collate_fn=collate_fn,\n    persistent_workers=use_persistent, prefetch_factor=prefetch_factor,\n    timeout=timeout_seconds, multiprocessing_context=mp_ctx\n)\n\nprint(f\"Workers={num_workers}, prefetch={prefetch_factor}, pin_memory={pin_mem}, start='fork', GPUs={world_size}\")\n\nfor _ in range(3):\n    _ = next(iter(dl_train))\nprint(\"DataLoader warmup complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:54:22.283703Z","iopub.execute_input":"2025-10-15T10:54:22.283975Z","iopub.status.idle":"2025-10-15T10:54:25.705686Z","shell.execute_reply.started":"2025-10-15T10:54:22.283953Z","shell.execute_reply":"2025-10-15T10:54:25.704518Z"}},"outputs":[{"name":"stdout","text":"Workers=4, prefetch=4, pin_memory=True, start='fork', GPUs=2\n✅ DataLoader warmup complete\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"class SinusoidalPositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 4000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(pos * div)\n        pe[:, 1::2] = torch.cos(pos * div)\n        self.register_buffer(\"pe\", pe)\n    def forward(self, x: torch.Tensor):\n        T = x.size(1)\n        return x + self.pe[:T].unsqueeze(0)\n\nclass Landmark2TextTransformer(nn.Module):\n    def __init__(self, in_feat: int, vocab_size: int,\n                 d_model=512, nhead=8, num_enc=6, num_dec=6, d_ff=2048, dropout=0.1):  # bigger model\n        super().__init__()\n        self.input_proj = nn.Linear(in_feat, d_model)\n        self.pos_enc_in = SinusoidalPositionalEncoding(d_model)\n        self.embed_out  = nn.Embedding(vocab_size, d_model, padding_idx=char2idx[PAD])\n        self.pos_enc_out = SinusoidalPositionalEncoding(d_model)\n        self.tf = nn.Transformer(\n            d_model=d_model, nhead=nhead,\n            num_encoder_layers=num_enc, num_decoder_layers=num_dec,\n            dim_feedforward=d_ff, dropout=dropout, batch_first=True\n        )\n        self.norm = nn.LayerNorm(d_model)\n        self.head = nn.Linear(d_model, vocab_size)\n    def forward(self, src, src_key_padding_mask, tgt, tgt_key_padding_mask):\n        src = self.pos_enc_in(self.input_proj(src))\n        tgt = self.pos_enc_out(self.embed_out(tgt))\n        L = tgt.size(1)\n        tgt_mask = nn.Transformer.generate_square_subsequent_mask(L, device=tgt.device)\n        out = self.tf(src=src, tgt=tgt,\n                      src_key_padding_mask=src_key_padding_mask,\n                      tgt_key_padding_mask=tgt_key_padding_mask,\n                      tgt_mask=tgt_mask)\n        out = self.norm(out)\n        return self.head(out)\n\nmodel = Landmark2TextTransformer(in_feat=n_features, vocab_size=vocab_size).to(DEVICE)\n\nif torch.cuda.is_available() and torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs via DataParallel\")\n    model = torch.nn.DataParallel(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:54:37.100121Z","iopub.execute_input":"2025-10-15T10:54:37.100932Z","iopub.status.idle":"2025-10-15T10:54:37.620963Z","shell.execute_reply.started":"2025-10-15T10:54:37.100898Z","shell.execute_reply":"2025-10-15T10:54:37.619964Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs via DataParallel\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"def _levenshtein(a: List[int], b: List[int]) -> int:\n    n, m = len(a), len(b)\n    dp = list(range(m+1))\n    for i in range(1, n+1):\n        prev, dp[0] = dp[0], i\n        for j in range(1, m+1):\n            prev, dp[j] = dp[j], min(\n                dp[j] + 1,\n                dp[j-1] + 1,\n                prev + (a[i-1] != b[j-1])\n            )\n    return dp[m]\n\ndef cer(ref_texts: List[str], hyp_texts: List[str]) -> float:\n    edits = total = 0\n    for r, h in zip(ref_texts, hyp_texts):\n        r_ids = [ord(c) for c in r]\n        h_ids = [ord(c) for c in h]\n        edits += _levenshtein(r_ids, h_ids)\n        total += max(1, len(r_ids))\n    return edits / total\n\nEPOCHS = 30                        \nLR     = 2e-4                     \ncriterion = nn.CrossEntropyLoss(ignore_index=char2idx[PAD], label_smoothing=0.1)\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01, betas=(0.9, 0.98))\nsteps_per_epoch = len(dl_train)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=LR, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)\nscaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n\ndef greedy_decode(model, src, src_mask, max_len=128):\n    model.eval()\n    B = src.size(0)\n    ys = torch.full((B, 1), fill_value=char2idx[BOS], dtype=torch.long, device=src.device)\n    with torch.no_grad():\n        for _ in range(max_len-1):\n            ymask = torch.zeros_like(ys, dtype=torch.bool, device=src.device)\n            logits = model(src, src_mask, ys, ymask)\n            next_tok = logits[:, -1].argmax(-1, keepdim=True)\n            ys = torch.cat([ys, next_tok], dim=1)\n            if torch.all(next_tok.squeeze(-1) == char2idx[EOS]):\n                break\n    return ys\n\ndef run_eval(model, dl):\n    model.eval()\n    refs, hyps = [], []\n    with torch.no_grad():\n        for xb, xmask, yb, ymask, _, _ in dl:\n            xb, xmask = xb.to(DEVICE), xmask.to(DEVICE)\n            yb = yb.to(DEVICE)\n            preds = greedy_decode(model, xb, xmask, max_len=yb.size(1)+10)\n            pred_txt = []\n            for row in preds.tolist():\n                s = [t for t in row if t not in (char2idx[PAD], char2idx[BOS])]\n                if char2idx[EOS] in s: s = s[:s.index(char2idx[EOS])]\n                pred_txt.append(ids_to_text(s))\n            gold_txt = []\n            for row in yb.tolist():\n                s = [t for t in row if t not in (char2idx[PAD], char2idx[BOS], char2idx[EOS])]\n                gold_txt.append(ids_to_text(s))\n            refs.extend(gold_txt); hyps.extend(pred_txt)\n    return cer(refs, hyps)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:54:47.067395Z","iopub.execute_input":"2025-10-15T10:54:47.067700Z","iopub.status.idle":"2025-10-15T10:54:47.089705Z","shell.execute_reply.started":"2025-10-15T10:54:47.067677Z","shell.execute_reply":"2025-10-15T10:54:47.088839Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"GRAD_ACCUM = 1\nCLIP_NORM  = 1.0\nSAVE_BEST  = OUT_DIR / \"model_phase2_best.pt\"\n\nprint(\"GPUs:\", torch.cuda.device_count(), \"| len(ds_train) =\", len(ds_train), \"| len(dl_train) =\", len(dl_train), \"| BATCH_SIZE =\", BATCH_SIZE)\nprint(\"approx samples/epoch =\", len(dl_train) * BATCH_SIZE)\n\nbest_cer = float(\"inf\")\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    t0 = time.time(); total_loss = 0.0\n\n    for step, (xb, xmask, yb, ymask, _, _) in enumerate(dl_train, start=1):\n        xb, xmask = xb.to(DEVICE, non_blocking=True), xmask.to(DEVICE, non_blocking=True)\n        yb, ymask = yb.to(DEVICE, non_blocking=True), ymask.to(DEVICE, non_blocking=True)\n\n        y_in, y_tgt = yb[:, :-1], yb[:, 1:]\n        ymask_in    = ymask[:, :-1]\n\n        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n            logits = model(xb, xmask, y_in, ymask_in)\n            loss = criterion(logits.reshape(-1, logits.size(-1)), y_tgt.reshape(-1))\n\n        scaler.scale(loss / GRAD_ACCUM).backward()\n        if step % GRAD_ACCUM == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n            scaler.step(optimizer); scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n            scheduler.step()\n\n        total_loss += loss.item()\n\n    val_cer = run_eval(model, dl_val)\n    took = time.time() - t0\n    print(f\"Epoch {epoch:02d} | train_loss={total_loss/len(dl_train):.4f} | val_CER={val_cer:.4f} | time={took/60:.2f} min\")\n\n    to_save = model.module if isinstance(model, torch.nn.DataParallel) else model\n    if val_cer < best_cer:\n        best_cer = val_cer\n        torch.save({\"model\": to_save.state_dict(),\n                    \"char2idx\": char2idx,\n                    \"idx2char\": idx2char,\n                    \"config\": {\"in_feat\": n_features, \"vocab_size\": vocab_size}}, SAVE_BEST)\n        print(f\"  ✓ New best CER {best_cer:.4f}. Saved -> {SAVE_BEST}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T10:55:06.408869Z","iopub.execute_input":"2025-10-15T10:55:06.409596Z","iopub.status.idle":"2025-10-15T14:56:50.151982Z","shell.execute_reply.started":"2025-10-15T10:55:06.409570Z","shell.execute_reply":"2025-10-15T14:56:50.150998Z"}},"outputs":[{"name":"stdout","text":"GPUs: 2 | len(ds_train) = 60485 | len(dl_train) = 473 | BATCH_SIZE = 128\napprox samples/epoch = 60544\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | train_loss=3.1788 | val_CER=0.9386 | time=5.34 min\n  ✓ New best CER 0.9386. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 02 | train_loss=2.8424 | val_CER=1.0225 | time=6.49 min\nEpoch 03 | train_loss=2.7291 | val_CER=1.0275 | time=7.43 min\nEpoch 04 | train_loss=2.6391 | val_CER=0.9655 | time=7.34 min\nEpoch 05 | train_loss=2.5728 | val_CER=0.9854 | time=7.62 min\nEpoch 06 | train_loss=2.5188 | val_CER=0.9434 | time=7.98 min\nEpoch 07 | train_loss=2.4664 | val_CER=0.9440 | time=7.89 min\nEpoch 08 | train_loss=2.4051 | val_CER=0.9273 | time=7.86 min\n  ✓ New best CER 0.9273. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 09 | train_loss=2.2999 | val_CER=0.8089 | time=8.32 min\n  ✓ New best CER 0.8089. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 10 | train_loss=2.1381 | val_CER=0.7239 | time=8.16 min\n  ✓ New best CER 0.7239. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 11 | train_loss=1.9683 | val_CER=0.6763 | time=8.06 min\n  ✓ New best CER 0.6763. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 12 | train_loss=1.8186 | val_CER=0.6034 | time=8.18 min\n  ✓ New best CER 0.6034. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 13 | train_loss=1.7024 | val_CER=0.5283 | time=8.29 min\n  ✓ New best CER 0.5283. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 14 | train_loss=1.6093 | val_CER=0.4841 | time=8.51 min\n  ✓ New best CER 0.4841. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 15 | train_loss=1.5354 | val_CER=0.4527 | time=8.39 min\n  ✓ New best CER 0.4527. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 16 | train_loss=1.4723 | val_CER=0.4456 | time=8.38 min\n  ✓ New best CER 0.4456. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 17 | train_loss=1.4180 | val_CER=0.4204 | time=8.52 min\n  ✓ New best CER 0.4204. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 18 | train_loss=1.3709 | val_CER=0.3974 | time=8.27 min\n  ✓ New best CER 0.3974. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 19 | train_loss=1.3271 | val_CER=0.3994 | time=8.45 min\nEpoch 20 | train_loss=1.2862 | val_CER=0.4005 | time=8.68 min\nEpoch 21 | train_loss=1.2489 | val_CER=0.3900 | time=8.33 min\n  ✓ New best CER 0.3900. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 22 | train_loss=1.2152 | val_CER=0.3812 | time=8.32 min\n  ✓ New best CER 0.3812. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 23 | train_loss=1.1843 | val_CER=0.3829 | time=8.44 min\nEpoch 24 | train_loss=1.1570 | val_CER=0.3827 | time=8.38 min\nEpoch 25 | train_loss=1.1334 | val_CER=0.3733 | time=8.34 min\n  ✓ New best CER 0.3733. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 26 | train_loss=1.1149 | val_CER=0.3716 | time=8.33 min\n  ✓ New best CER 0.3716. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 27 | train_loss=1.1001 | val_CER=0.3717 | time=8.35 min\nEpoch 28 | train_loss=1.0898 | val_CER=0.3692 | time=8.30 min\n  ✓ New best CER 0.3692. Saved -> /kaggle/working/asl_phase1/model_phase2_best.pt\nEpoch 29 | train_loss=1.0835 | val_CER=0.3701 | time=8.29 min\nEpoch 30 | train_loss=1.0810 | val_CER=0.3707 | time=8.33 min\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"import os, math, time, collections\nfrom pathlib import Path\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nOUT_DIR = Path(\"/kaggle/working/asl_phase1\")\nCKPT = OUT_DIR / \"model_phase2_best.pt\"\nassert CKPT.exists(), f\"Missing checkpoint at {CKPT}; finish Phase 2 training first.\"\n\nif 'char2idx' not in globals() or 'idx2char' not in globals():\n    blob = torch.load(CKPT, map_location=\"cpu\")\n    char2idx = blob[\"char2idx\"]\n    idx2char = blob[\"idx2char\"]\n\nPAD = next((k for k,v in char2idx.items() if v == 0), \"<pad>\")\nif PAD not in char2idx: char2idx[PAD] = 0\nif \"<bos>\" not in char2idx:\n    char2idx[\"<bos>\"] = max(char2idx.values()) + 1\nif \"<eos>\" not in char2idx:\n    char2idx[\"<eos>\"] = max(char2idx.values()) + 1\nidx2char = {i:c for c,i in char2idx.items()}\nPAD_ID, BOS_ID, EOS_ID = char2idx[PAD], char2idx[\"<bos>\"], char2idx[\"<eos>\"]\n\nif 'Landmark2TextTransformer' not in globals():\n    class SinusoidalPositionalEncoding(nn.Module):\n        def __init__(self, d_model: int, max_len: int = 4000):\n            super().__init__()\n            pe = torch.zeros(max_len, d_model)\n            pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n            div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n            pe[:, 0::2] = torch.sin(pos * div)\n            pe[:, 1::2] = torch.cos(pos * div)\n            self.register_buffer(\"pe\", pe)\n        def forward(self, x: torch.Tensor):\n            T = x.size(1)\n            return x + self.pe[:T].unsqueeze(0)\n\n    class Landmark2TextTransformer(nn.Module):\n        def __init__(self, in_feat: int, vocab_size: int,\n                     d_model=512, nhead=8, num_enc=6, num_dec=6, d_ff=2048, dropout=0.1):\n            super().__init__()\n            self.input_proj = nn.Linear(in_feat, d_model)\n            self.pos_enc_in = SinusoidalPositionalEncoding(d_model)\n            self.embed_out  = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n            self.pos_enc_out = SinusoidalPositionalEncoding(d_model)\n            self.tf = nn.Transformer(\n                d_model=d_model, nhead=nhead,\n                num_encoder_layers=num_enc, num_decoder_layers=num_dec,\n                dim_feedforward=d_ff, dropout=dropout, batch_first=True\n            )\n            self.norm = nn.LayerNorm(d_model)\n            self.head = nn.Linear(d_model, vocab_size)\n        def forward(self, src, src_key_padding_mask, tgt, tgt_key_padding_mask):\n            src = self.pos_enc_in(self.input_proj(src))\n            tgt = self.pos_enc_out(self.embed_out(tgt))\n            L = tgt.size(1)\n            tgt_mask = nn.Transformer.generate_square_subsequent_mask(L, device=tgt.device)\n            out = self.tf(src=src, tgt=tgt,\n                          src_key_padding_mask=src_key_padding_mask,\n                          tgt_key_padding_mask=tgt_key_padding_mask,\n                          tgt_mask=tgt_mask)\n            out = self.norm(out)\n            return self.head(out)\n\nblob = torch.load(CKPT, map_location=\"cpu\")\ncfg = blob.get(\"config\", {})\nin_feat = cfg.get(\"in_feat\", None)\nvocab_size = cfg.get(\"vocab_size\", len(char2idx))\nassert in_feat is not None, \"Checkpoint missing in_feat; was it saved?\"\n\nmodel = Landmark2TextTransformer(in_feat=in_feat, vocab_size=vocab_size).to(DEVICE)\nmodel.load_state_dict(blob[\"model\"], strict=True)\nmodel.eval()\nprint(\"✓ Loaded Phase 2 best checkpoint:\", CKPT)\nprint(\"in_feat =\", in_feat, \"| vocab_size =\", vocab_size, \"| device =\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T15:17:29.503097Z","iopub.execute_input":"2025-10-15T15:17:29.504369Z","iopub.status.idle":"2025-10-15T15:17:30.297924Z","shell.execute_reply.started":"2025-10-15T15:17:29.504337Z","shell.execute_reply":"2025-10-15T15:17:30.296977Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded Phase 2 best checkpoint: /kaggle/working/asl_phase1/model_phase2_best.pt\nin_feat = 144 | vocab_size = 62 | device = cuda\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"import cv2\nimport mediapipe as mp\nmp_hands = mp.solutions.hands\nmp_pose  = mp.solutions.pose\n\nPOSE_KEEP = [11, 12, 13, 14, 15, 16]\nLH_IDX = list(range(21))\nRH_IDX = list(range(21))\n\ndef _safe_arr3(v):\n    try:\n        return np.array([float(v.x), float(v.y), float(getattr(v, 'z', 0.0))], dtype=np.float32)\n    except Exception:\n        return None\n\ndef extract_landmarks(frame_bgr):\n    img = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    img.flags.writeable = False\n    pose_results = extract_landmarks.pose.process(img)\n    hands_results = extract_landmarks.hands.process(img)\n    img.flags.writeable = True\n\n    out = {'pose': {}, 'left': {}, 'right': {}}\n    if pose_results.pose_landmarks:\n        plms = pose_results.pose_landmarks.landmark\n        for i in POSE_KEEP:\n            v = _safe_arr3(plms[i])\n            if v is not None:\n                out['pose'][i] = v\n\n    if hands_results.multi_hand_landmarks and hands_results.multi_handedness:\n        for hand_lms, handed in zip(hands_results.multi_hand_landmarks, hands_results.multi_handedness):\n            label = handed.classification[0].label.lower()  # 'left' or 'right'\n            pts = hand_lms.landmark\n            for j in range(21):\n                v = _safe_arr3(pts[j])\n                if v is not None:\n                    out[label][j] = v\n    return out\n\nextract_landmarks.pose  = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False)\nextract_landmarks.hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n\ndef normalize_frame(det):\n    have_pose = (11 in det['pose']) and (12 in det['pose'])\n    if have_pose:\n        lsh, rsh = det['pose'][11], det['pose'][12]\n        center = (lsh + rsh) / 2.0\n        scale = np.linalg.norm(lsh - rsh)\n    else:\n        lw = det['left'].get(0, None)\n        rw = det['right'].get(0, None)\n        if lw is None and rw is None:\n            center = np.zeros(3, dtype=np.float32)\n            scale  = 1e-6\n        else:\n            if lw is None: lw = rw\n            if rw is None: rw = lw\n            center = (lw + rw) / 2.0\n            scale  = np.linalg.norm(lw - rw)\n    scale = max(scale, 1e-6)\n\n    feats = []\n    for i in POSE_KEEP:\n        xyz = det['pose'].get(i, np.zeros(3, dtype=np.float32))\n        n = (xyz - center) / scale\n        feats.extend(n.tolist())\n    for j in LH_IDX:\n        xyz = det['left'].get(j, np.zeros(3, dtype=np.float32))\n        n = (xyz - center) / scale\n        feats.extend(n.tolist())\n    for j in RH_IDX:\n        xyz = det['right'].get(j, np.zeros(3, dtype=np.float32))\n        n = (xyz - center) / scale\n        feats.extend(n.tolist())\n\n    x = np.asarray(feats, dtype=np.float32)\n    np.nan_to_num(x, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n    np.clip(x, -1e6, 1e6, out=x)\n    return x\n\nEXPECTED_IN_FEAT = (len(POSE_KEEP) + len(LH_IDX) + len(RH_IDX)) * 3\nassert EXPECTED_IN_FEAT == in_feat, f\"Feature dim mismatch: expected {EXPECTED_IN_FEAT} from MP, ckpt needs {in_feat}\"\nprint(\"✓ MediaPipe feature dim OK =\", EXPECTED_IN_FEAT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T15:18:06.267826Z","iopub.execute_input":"2025-10-15T15:18:06.268128Z","iopub.status.idle":"2025-10-15T15:18:06.300676Z","shell.execute_reply.started":"2025-10-15T15:18:06.268106Z","shell.execute_reply":"2025-10-15T15:18:06.298336Z"}},"outputs":[{"name":"stdout","text":"✓ MediaPipe feature dim OK = 144\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1760541486.358401  131159 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1760541486.390062  131159 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1760541486.420030  131156 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1760541486.482399  131156 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"from collections import deque\n\nclass TemporalSmoother:\n    \"\"\"EMA/Moving-average smoothing over next-token probabilities to reduce jitter.\"\"\"\n    def __init__(self, vocab_size, k=5, ema=False, alpha=0.4):\n        self.vocab_size = vocab_size\n        self.k = k\n        self.ema = ema\n        self.alpha = alpha\n        self.buf = deque(maxlen=k)\n        self._ema_vec = None\n\n    def update(self, probs: torch.Tensor) -> np.ndarray:\n        p = probs.detach().cpu().float().numpy()\n        if self.ema:\n            if self._ema_vec is None:\n                self._ema_vec = p.copy()\n            else:\n                self._ema_vec = self.alpha * p + (1 - self.alpha) * self._ema_vec\n            return self._ema_vec\n        else:\n            self.buf.append(p)\n            return np.mean(self.buf, axis=0) if len(self.buf) else p\n\nclass RealTimeDecoder:\n    def __init__(self, model, bos_id, eos_id, pad_id, window=64, smooth_k=5, commit_thresh=0.60, commit_patience=3):\n        self.model = model\n        self.window = window\n        self.queue = deque(maxlen=window)\n        self.bos_id, self.eos_id, self.pad_id = bos_id, eos_id, pad_id\n        self.smoother = TemporalSmoother(vocab_size=vocab_size, k=smooth_k)\n        self.commit_thresh = commit_thresh\n        self.commit_patience = commit_patience\n        self._cand_id = None\n        self._cand_count = 0\n        self.committed = []  \n\n    def reset_text(self):\n        self.committed.clear()\n        self._cand_id, self._cand_count = None, 0\n        self.smoother.buf.clear()\n\n    def push_frame(self, x_feat: np.ndarray):\n        assert x_feat.shape[0] == in_feat\n        self.queue.append(x_feat)\n\n    @torch.no_grad()\n    def step(self):\n        if len(self.queue) < 4:\n            return None, \"\".join(idx2char.get(t, \"\") for t in self.committed)\n\n        frames = np.stack(self.queue, axis=0)\n        src = torch.from_numpy(frames).unsqueeze(0).to(DEVICE)  \n        src_mask = torch.zeros((1, src.size(1)), dtype=torch.bool, device=DEVICE)  \n\n        ys = torch.tensor([[self.bos_id] + self.committed], dtype=torch.long, device=DEVICE)\n        ymask = torch.zeros_like(ys, dtype=torch.bool, device=DEVICE)\n\n        logits = self.model(src, src_mask, ys, ymask)     \n        next_logits = logits[0, -1]                       \n        probs = F.softmax(next_logits, dim=-1)            \n        smoothed = self.smoother.update(probs)           \n\n        top_id = int(np.argmax(smoothed))\n        top_p  = float(smoothed[top_id])\n\n        if top_id == self._cand_id:\n            self._cand_count += 1\n        else:\n            self._cand_id = top_id\n            self._cand_count = 1\n\n        committed_changed = False\n        if top_id not in (self.pad_id, self.bos_id) and top_p >= self.commit_thresh and self._cand_count >= self.commit_patience:\n            if not self.committed or self.committed[-1] != top_id:\n                if top_id == self.eos_id:\n                    pass\n                else:\n                    self.committed.append(top_id)\n                    committed_changed = True\n            self._cand_count = 0\n\n        text = \"\".join(idx2char.get(t, \"\") for t in self.committed if t not in (self.pad_id, self.bos_id, self.eos_id))\n        return (top_id, top_p, committed_changed), text\n\ndecoder = RealTimeDecoder(model, BOS_ID, EOS_ID, PAD_ID, window=64, smooth_k=5, commit_thresh=0.60, commit_patience=3)\nprint(\"✓ RealTimeDecoder ready (window=64, smoothing K=5, thresh=0.60, patience=3)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T15:18:19.723480Z","iopub.execute_input":"2025-10-15T15:18:19.723921Z","iopub.status.idle":"2025-10-15T15:18:19.738947Z","shell.execute_reply.started":"2025-10-15T15:18:19.723886Z","shell.execute_reply":"2025-10-15T15:18:19.738038Z"}},"outputs":[{"name":"stdout","text":"✓ RealTimeDecoder ready (window=64, smoothing K=5, thresh=0.60, patience=3)\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"VIDEO_PATH = 0  \nFONT = cv2.FONT_HERSHEY_SIMPLEX\n\ndef run_realtime(source=VIDEO_PATH, flip=True, target_fps=20):\n    cap = cv2.VideoCapture(source)\n    if not cap.isOpened():\n        raise RuntimeError(\"Could not open video source. Set VIDEO_PATH to a video file path.\")\n\n    decoder.reset_text()\n    last = time.time()\n    frame_interval = 1.0 / max(1, target_fps)\n\n    try:\n        while True:\n            ok, frame = cap.read()\n            if not ok: break\n            if flip: frame = cv2.flip(frame, 1)\n\n            det = extract_landmarks(frame)\n            x = normalize_frame(det)\n            decoder.push_frame(x)\n            _, text = decoder.step()\n\n            cv2.putText(frame, text, (20, 40), FONT, 1.0, (0, 255, 0), 2, cv2.LINE_AA)\n\n            now = time.time()\n            if now - last >= frame_interval:\n                last = now\n                cv2.imshow(\"ASL Fingerspelling (Phase 3 - smoothed)\", frame)\n            if cv2.waitKey(1) & 0xFF == 27:  # ESC\n                break\n    finally:\n        cap.release()\n        cv2.destroyAllWindows()\n\nprint(\"✓ Live loop ready. Call run_realtime() to test (webcam) or run_realtime('/path/to/video.mp4').\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T15:18:37.021501Z","iopub.execute_input":"2025-10-15T15:18:37.022092Z","iopub.status.idle":"2025-10-15T15:18:37.029236Z","shell.execute_reply.started":"2025-10-15T15:18:37.022064Z","shell.execute_reply":"2025-10-15T15:18:37.028450Z"}},"outputs":[{"name":"stdout","text":"✓ Live loop ready. Call run_realtime() to test (webcam) or run_realtime('/path/to/video.mp4').\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"def infer_on_image(bgr_img, warmup=32):\n    decoder.reset_text()\n    det = extract_landmarks(bgr_img)\n    x = normalize_frame(det)\n    for _ in range(warmup):\n        decoder.push_frame(x)\n        decoder.step()\n    _, text = decoder.step()\n    return text\n\nprint(\"✓ Single-frame function ready: infer_on_image(bgr_img)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T15:19:00.673381Z","iopub.execute_input":"2025-10-15T15:19:00.673667Z","iopub.status.idle":"2025-10-15T15:19:00.679247Z","shell.execute_reply.started":"2025-10-15T15:19:00.673648Z","shell.execute_reply":"2025-10-15T15:19:00.678142Z"}},"outputs":[{"name":"stdout","text":"✓ Single-frame function ready: infer_on_image(bgr_img)\n","output_type":"stream"}],"execution_count":96}]}